{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d661f4",
   "metadata": {},
   "source": [
    "You're absolutely right — checking **unique values** is a *critical* part of data cleaning, and I should have emphasized it more clearly. Here's how it fits into the process:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Updated Data Cleaning Workflow (with Unique Value Checks)\n",
    "\n",
    "#### **1. Understand the Data Structure**\n",
    "\n",
    "* `df.shape`, `df.info()`, `df.describe(include='all')`\n",
    "* `df.dtypes` to inspect types\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Check Unique Values**\n",
    "\n",
    "Check *before* cleaning missing values, to:\n",
    "\n",
    "* Spot unexpected or inconsistent categories\n",
    "* See if a column is constant or low-cardinality (often a candidate for dropping or encoding)\n",
    "\n",
    "**What to do:**\n",
    "\n",
    "```python\n",
    "for col in df.columns:\n",
    "  print(f\"\\nColumn: {col}\")\n",
    "  print(df[col].value_counts(dropna=False).head(10))  # Show top 10 values incl. NaN\n",
    "  print(f\"Unique values: {df[col].nunique(dropna=False)}\")\n",
    "```\n",
    "\n",
    "✅ Focus on:\n",
    "\n",
    "* **Categorical/text columns**: inconsistent labels, typos, mixed casing\n",
    "* **Numeric columns**: unrealistic values (e.g., negative freight cost)\n",
    "* **Datetime columns**: strange formats, default timestamps (e.g., 1970-01-01)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Handle Column Types**\n",
    "\n",
    "Convert types (`datetime`, `category`, etc.) once you're sure values are consistent.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Handle Missing Values**\n",
    "\n",
    "Now that you understand the unique values and distribution, clean them properly:\n",
    "\n",
    "* Drop, fill, or flag based on meaning\n",
    "* Use `.isna().sum()` again after changes\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Fix Duplicates**\n",
    "\n",
    "* `.duplicated()` and `.drop_duplicates()`\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Standardize Text/Categories**\n",
    "\n",
    "* Lowercase, strip whitespace\n",
    "* Rename or group rare categories\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Feature Engineering**\n",
    "\n",
    "* Date/time decomposition\n",
    "* Flags from `NaN`\n",
    "* Duration calculations\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Filter Bad Rows**\n",
    "\n",
    "* Invalid status\n",
    "* Unrealistic entries\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Save & Repeat for Other Tables**\n",
    "\n",
    "---\n",
    "\n",
    "**Checking unique values is the diagnostic step that prevents bad assumptions.** Thanks for pointing it out — it's a step you **always** want in the top 3 of your cleaning process.\n",
    "\n",
    "Would you like me to generate a reusable cleaning template with unique-value checking built in?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
